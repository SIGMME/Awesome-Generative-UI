<div align="center">
    <img alt="logo" src="./images/logo.png" style="height: 200px;" />
</div>


<div align="center">

# Awesome Generative UI

</div>


We aim to provide the community with a comprehensive and timely synthesis of this fascinating and promising field, as well as some insights into it. This repository provides valuable reference for researchers in the field of Generative UI, please start your exploration! 

***This work is in progress!***

---

<font size=5><center><b> Table of Contents </b> </center></font>

<!-- - [Our Group](#our-group)
  - [Originators](#originators)
  - [Members](#members)
- [Our Activities](#our-activities) -->
<!-- - [Awesome Examples](#Awesome-genui-examples) -->
- [Awesome Generative UI](#awesome-generative-ui)
  - [Awesome Generative UI Papers](#awesome-generative-ui-papers)
  - [Awesome Datasets \& Benchmarks](#awesome-datasets--benchmarks)


---

## Awesome Generative UI Papers

* [2511] [AUI] [Computer-Use Agents as Judges for Generative User Interface](https://arxiv.org/pdf/2511.15567) [[ðŸ’»Code](https://github.com/showlab/AUI)] [[ðŸ¤—Model](https://huggingface.co/spaces/showlab/AUI)]

  
* [2511] [UI2Code^N] [UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation](https://arxiv.org/pdf/2511.08195) [[ðŸ’»Code](https://github.com/zai-org/UI2Code_N)] [[ðŸ¤—Model](https://huggingface.co/zai-org/UI2Code_N)]

* [2511] [WebVIA] [WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation](https://arxiv.org/pdf/2511.06251) [[ðŸ’»Code](https://github.com/zheny2751-dotcom/WebVIA)] [[ðŸ¤—Model](https://huggingface.co/zai-org/WebVIA-Agent)]

* [2511] [Portal UX Agent] [Portal UX Agent -- A Plug-and-Play Engine for Rendering UIs from Natural Language Specifications](https://arxiv.org/pdf/2511.00843)

* [2510] [Code Aesthetics] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/pdf/2510.23272) [[ðŸ¤—Model](https://huggingface.co/SamuelBang/AesCoder-4B)]

* [2510] [Relook] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/pdf/2510.11498)

* [2509] [UI-UG] [UI-UG: A Unified MLLM for UI Understanding and Generation](https://arxiv.org/pdf/2509.24361) [[ðŸ’»Code](https://github.com/neovateai/UI-UG)] [[ðŸ¤—Model](https://huggingface.co/neovateai/UI-UG-7B)]

* [2508] [LaTCoder] [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/pdf/2508.03560) 

* [2506] [LayoutCoder] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/pdf/2506.10376)

---

## Awesome Datasets & Benchmarks
  
* [**Image2code**] [2510] [WebRenderBench] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097) [[ðŸ¤—Datasets](https://huggingface.co/datasets/aleversn/WebRenderBench)]

* [**Video2code**] [2509] [IWR-Bench] [IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?](https://arxiv.org/abs/2509.24709) [[ðŸ’»Code](https://github.com/SIGMME/IWR-Bench)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/IWR-Bench/IWR-Bench)]

* [**Image2code**] [2508] [VisCodex] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945) [[ðŸ¤—Datasets](https://huggingface.co/datasets/lingjie23/MultimodalCodingDataset)]

* [**Image2code**] [2507] [ScreenCoder] [ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827) [[ðŸ’»Code](https://github.com/leigest519/ScreenCoder)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/Leigest/ScreenCoder)]
  
* [**Image2code**] [2506] [WebUIBench] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://www.arxiv.org/abs/2506.07818v1) [[ðŸ’»Code](https://github.com/MAIL-Tele-AI/WebUIBench)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/Tele-AI-MAIL/WebUIBench)]

* [**Image2code**] [2505] [FullFront] [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399) [[ðŸ’»Code](https://github.com/Mikivishy/FullFront)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/Mikivis/FullFront)]
  
* [**Image2code**] [2505] [WebGen-Bench] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733) [[ðŸ¤—Datasets](https://huggingface.co/datasets/luzimu/WebGen-Bench)]

* [**Image2code**] [2406] [Web2Code] [Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs](https://arxiv.org/abs/2406.20098) [[ðŸ’»Code](https://github.com/MBZUAI-LLM/Web2code)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/MBZUAI/Web2Code)]

* [**Image2code**] [2404] [WebCode2M] [WebCode2M: AReal-World Dataset for Code Generation fromWebpage Designs](https://arxiv.org/abs/2404.06369) [[ðŸ¤—Datasets](https://huggingface.co/datasets/xcodemind/webcode2m)]

* [**Image2code**] [2403] [WebSight] [Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset](https://arxiv.org/abs/2403.09029) [[ðŸ¤—Datasets](https://huggingface.co/datasets/HuggingFaceM4/WebSight)]

* [**Image2code**] [2403] [Design2Code] [Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering](https://arxiv.org/abs/2403.03163) [[ðŸ’»Code](https://github.com/NoviScl/Design2Code)] [[ðŸ¤—Datasets](https://huggingface.co/datasets/SALT-NLP/Design2Code)]

